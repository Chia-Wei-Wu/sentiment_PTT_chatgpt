{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chia-Wei-Wu/sentiment_PTT_chatgpt/blob/main/techjob_TFIDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgTs5fWfmycW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pandas import json_normalize\n",
        "import pandas as pd\n",
        "import jieba\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aPlwn5OQm5AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_frame = pd.DataFrame(columns=[\"title\",\"author\",\"date\",\"push\",\"url\",\"content\"])"
      ],
      "metadata": {
        "id": "k0s-4sqxm6So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You need to connect the correct DB\n",
        "with open('/content/drive/MyDrive/AI_Final/tec_2023.json','r',encoding=\"utf-8\") as read_file:\n",
        "  lines = read_file.readlines()\n",
        "  for line in lines:\n",
        "    data_dic = json.loads(line)\n",
        "    data_row = pd.DataFrame([data_dic])\n",
        "    data_frame = pd.concat([data_frame, data_row],ignore_index=True)\n",
        "#print(data_frame)"
      ],
      "metadata": {
        "id": "yid4t_9nm-Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classification by date\n",
        "data_frame[\"date\"] = pd.to_datetime(data_frame[\"date\"], format=\" %m/%d\")\n",
        "Jan_DF = []\n",
        "Feb_DF = []\n",
        "Mar_DF = []\n",
        "Apr_DF = []\n",
        "count_jan = 0\n",
        "count_feb = 0\n",
        "count_mar = 0\n",
        "count_apr = 0\n",
        "for index, row in data_frame.iterrows():\n",
        "  if row[\"date\"].month == 1:\n",
        "    Jan_DF.append(row)\n",
        "    count_jan += 1\n",
        "  elif row[\"date\"].month == 2:\n",
        "    Feb_DF.append(row)\n",
        "    count_feb += 1\n",
        "  elif row[\"date\"].month == 3:\n",
        "    Mar_DF.append(row)\n",
        "    count_mar += 1\n",
        "  elif row[\"date\"].month == 4:\n",
        "    Apr_DF.append(row)\n",
        "    count_apr += 1"
      ],
      "metadata": {
        "id": "Cs5YceHDnAEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(count_jan,count_feb,count_mar,count_apr)"
      ],
      "metadata": {
        "id": "myKh4VPNnCfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bad_word = [\"\\n\",\"他\",\"的\",\"是\",\"了\",'等','(','／','高','3','更'')','/',')','包括','到','成為','相關','重要','處理','預計','他們','署名','進行','下','多','主要','整合','原文','報導','發布','可','被','從','使用','心得','-','人','已經','內容','標題','連結','如此','來','無','背','碼','元','以及','再者','尤其','%','一個','讓','最','複','僅','據','之道','指出','方面','億','目前','長','這一','總','得','個','以及','將','設','e','旨在','已','仍','鏈','具有','向','需','誠','其','一部分','比','第','寫','像','以來','佈','這些','內','中','成','誰','一些','以','就是','後','一種','哪些','它','由','很','而','較',\"在\",\"也\",\"對\",\"與\",\"和\",\"之\",\"為\",\"於\",\"來\",\"有\",\"就\",\"我\",\"不\",\"要\",\"跟\",\"說\",\"都\",\"你\",'造',\"可以\",\"會\",\"可能\",\"做\",\"但是\",\"沒\",\"但\",\"上\",'製']"
      ],
      "metadata": {
        "id": "NNwPjBXanFaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with Jan\n",
        "content_col_jan = []\n",
        "for row in Jan_DF:\n",
        "  content_col_jan.append(row[\"content\"])\n",
        "\n",
        "result_content_jan = []\n",
        "for row in content_col_jan:\n",
        "  row_content_jan = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_jan.append(word)\n",
        "  result_content_jan.append(row_content_jan)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_jan = []\n",
        "count_jan_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_jan:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_jan.append(row)\n",
        "    count_jan_GPT += 1 \n",
        "\n",
        "#delete stopword\n",
        "for bad in bad_word:\n",
        "  for sublist in having_GPT_jan:\n",
        "    while bad in sublist:\n",
        "      sublist.remove(bad)"
      ],
      "metadata": {
        "id": "3MXdyld1nH3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with Feb\n",
        "content_col_feb = []\n",
        "for row in Feb_DF:\n",
        "  content_col_feb.append(row[\"content\"])\n",
        "\n",
        "result_content_feb = []\n",
        "for row in content_col_feb:\n",
        "  row_content_feb = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_feb.append(word)\n",
        "  result_content_feb.append(row_content_feb)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_feb = []\n",
        "count_feb_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_feb:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_feb.append(row)\n",
        "    count_feb_GPT += 1\n",
        "\n",
        "#delete stopword\n",
        "for bad in bad_word:\n",
        "  for sublist in having_GPT_feb:\n",
        "    while bad in sublist:\n",
        "      sublist.remove(bad)"
      ],
      "metadata": {
        "id": "dCXUwekHnJrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with mar\n",
        "content_col_mar = []\n",
        "for row in Mar_DF:\n",
        "  content_col_mar.append(row[\"content\"])\n",
        "\n",
        "result_content_mar = []\n",
        "for row in content_col_mar:\n",
        "  row_content_mar = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_mar.append(word)\n",
        "  result_content_mar.append(row_content_mar)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_mar = []\n",
        "count_mar_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_mar:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_mar.append(row)\n",
        "    count_mar_GPT += 1\n",
        "\n",
        "#delete stopword\n",
        "for bad in bad_word:\n",
        "  for sublist in having_GPT_mar:\n",
        "    while bad in sublist:\n",
        "      sublist.remove(bad)"
      ],
      "metadata": {
        "id": "qy7TbUFFnMn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with apr\n",
        "content_col_apr = []\n",
        "for row in Apr_DF:\n",
        "  content_col_apr.append(row[\"content\"])\n",
        "\n",
        "result_content_apr = []\n",
        "for row in content_col_apr:\n",
        "  row_content_apr = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_apr.append(word)\n",
        "  result_content_apr.append(row_content_apr)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_apr = []\n",
        "count_apr_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_mar:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_apr.append(row)\n",
        "    count_apr_GPT += 1\n",
        "\n",
        "#delete stopword\n",
        "for bad in bad_word:\n",
        "  for sublist in having_GPT_apr:\n",
        "    while bad in sublist:\n",
        "      sublist.remove(bad)"
      ],
      "metadata": {
        "id": "PWjm4pzhnQIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt \n",
        "width=1280\n",
        "height=1024\n",
        "bg='white'\n",
        "angle=0.9\n",
        "myfont='/content/drive/MyDrive/AI_Final/KAIU.TTF'"
      ],
      "metadata": {
        "id": "IKtA1zP2nShv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF "
      ],
      "metadata": {
        "id": "j8FxtGa3nXpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanfordcorenlp\n",
        "!tar xzvf stanford-corenlp-full-2018-02-27.tgz"
      ],
      "metadata": {
        "id": "YmrvCvVhnVUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Strcontent_col_jan = \",\".join(content_col_jan)\n",
        "Strcontent_col_feb = \",\".join(content_col_feb)\n",
        "Strcontent_col_mar = \",\".join(content_col_mar)\n",
        "Strcontent_col_apr = \",\".join(content_col_apr)"
      ],
      "metadata": {
        "id": "gM9AEmbvnZ7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str_ann = '[〔〕；，、。：「『《（」』》）？]|\\[. *?\\]n'\n",
        "def filterSentence_v2(str_sent):\n",
        "    # keep only word\n",
        "    #step-1: filter \n",
        "    str_Filtered = re.sub(str_ann, ' ', str_sent)\n",
        "    print(str_Filtered)\n",
        "    return str_Filtered"
      ],
      "metadata": {
        "id": "rb9XEKpmncDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   ---Jan---"
      ],
      "metadata": {
        "id": "u3qGJyaCnjEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Content_LibTime = Strcontent_col_jan\n",
        "word_result = [item for sublist in having_GPT_jan for item in sublist]\n",
        "#print(word_result)\n",
        "text = ' '.join(word_result)\n",
        "\n",
        "# Compute TF\n",
        "stat_dict = {}\n",
        "\n",
        "for token in word_result:\n",
        "    if token in str_ann :\n",
        "        continue\n",
        "    if token in stat_dict:\n",
        "        stat_dict[token] = stat_dict[token] + 1\n",
        "    else:\n",
        "        stat_dict[token] = 1\n",
        "\n",
        "new_dict = sorted(stat_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "print('只依TF排序', '-'*50)\n",
        "print(new_dict)"
      ],
      "metadata": {
        "id": "J_YmjVxcnfwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute IDF\n",
        "corpus_list = [Strcontent_col_jan, Strcontent_col_feb, Strcontent_col_mar, Strcontent_col_apr]\n",
        "\n",
        "idf_dict={}\n",
        "N=len(corpus_list)\n",
        "for key in stat_dict:\n",
        "    count=0\n",
        "    for sen in corpus_list:\n",
        "        if key in sen.split():\n",
        "            count=count+1\n",
        "        idf_dict[key]= math.log10(N / (float(count) + 1))\n",
        "\n",
        "print('只依IDF排序', '-'*50)\n",
        "new_dict = sorted(idf_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "print(new_dict)"
      ],
      "metadata": {
        "id": "h0UStw7Knpjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute TF-IDF\n",
        "tfidf_dict = {}\n",
        "\n",
        "for key in stat_dict:\n",
        "    tfidf_dict[key] = stat_dict[key]*idf_dict[key]\n",
        "\n",
        "new_dict_jan = sorted(tfidf_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "print('依TF-IDF排序', '-'*50)\n",
        "print(new_dict_jan)"
      ],
      "metadata": {
        "id": "35QnhIyKnr0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_text = dict(new_dict_jan)\n",
        "cloud = WordCloud(background_color = bg, width=width, height=height, font_path=myfont).generate_from_frequencies(dict_text)\n",
        "\n",
        "plt.imshow(cloud)"
      ],
      "metadata": {
        "id": "zDaI7at6n5X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ---Feb---"
      ],
      "metadata": {
        "id": "an8iFRtJn9kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Content_LibTime = Strcontent_col_feb\n",
        "word_result = [item for sublist in having_GPT_feb for item in sublist]\n",
        "#print(word_result)\n",
        "text = ' '.join(word_result)\n",
        "\n",
        "# Compute TF\n",
        "stat_dict = {}\n",
        "\n",
        "for token in word_result:\n",
        "    if token in str_ann :\n",
        "        continue\n",
        "    if token in stat_dict:\n",
        "        stat_dict[token] = stat_dict[token] + 1\n",
        "    else:\n",
        "        stat_dict[token] = 1\n",
        "\n",
        "new_dict = sorted(stat_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "print('只依TF排序', '-'*50)\n",
        "print(new_dict)"
      ],
      "metadata": {
        "id": "9izE3cUqn7a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute IDF\n",
        "corpus_list = [Strcontent_col_jan, Strcontent_col_feb, Strcontent_col_mar, Strcontent_col_apr]\n",
        "\n",
        "idf_dict={}\n",
        "N=len(corpus_list)\n",
        "for key in stat_dict:\n",
        "    count=0\n",
        "    for sen in corpus_list:\n",
        "        if key in sen.split():\n",
        "            count=count+1\n",
        "        idf_dict[key]= math.log10(N / (float(count) + 1))\n",
        "\n",
        "print('只依IDF排序', '-'*50)\n",
        "new_dict = sorted(idf_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "print(new_dict)"
      ],
      "metadata": {
        "id": "f40jPzXSn_69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute TF-IDF\n",
        "tfidf_dict = {}\n",
        "\n",
        "for key in stat_dict:\n",
        "    tfidf_dict[key] = stat_dict[key]*idf_dict[key]\n",
        "\n",
        "new_dict_feb = sorted(tfidf_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "print('依TF-IDF排序', '-'*50)\n",
        "print(new_dict_feb)"
      ],
      "metadata": {
        "id": "mUBVXevBoCB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_text = dict(new_dict_feb)\n",
        "cloud = WordCloud(background_color = bg, width=width, height=height, font_path=myfont).generate_from_frequencies(dict_text)\n",
        "\n",
        "plt.imshow(cloud)"
      ],
      "metadata": {
        "id": "H4gSKl84oEOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --Mar---"
      ],
      "metadata": {
        "id": "788t3u8IoNpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Content_LibTime = Strcontent_col_mar\n",
        "word_result = [item for sublist in having_GPT_mar for item in sublist]\n",
        "#print(word_result)\n",
        "text = ' '.join(word_result)\n",
        "\n",
        "# Compute TF\n",
        "stat_dict = {}\n",
        "\n",
        "for token in word_result:\n",
        "    if token in str_ann :\n",
        "        continue\n",
        "    if token in stat_dict:\n",
        "        stat_dict[token] = stat_dict[token] + 1\n",
        "    else:\n",
        "        stat_dict[token] = 1\n",
        "\n",
        "new_dict = sorted(stat_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "print('只依TF排序', '-'*50)\n",
        "print(new_dict)"
      ],
      "metadata": {
        "id": "Nh73IAZIoLnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute IDF\n",
        "corpus_list = [Strcontent_col_jan, Strcontent_col_feb, Strcontent_col_mar, Strcontent_col_apr]\n",
        "\n",
        "idf_dict={}\n",
        "N=len(corpus_list)\n",
        "for key in stat_dict:\n",
        "    count=0\n",
        "    for sen in corpus_list:\n",
        "        if key in sen.split():\n",
        "            count=count+1\n",
        "        idf_dict[key]= math.log10(N / (float(count) + 1))\n",
        "\n",
        "print('只依IDF排序', '-'*50)\n",
        "new_dict = sorted(idf_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "print(new_dict)"
      ],
      "metadata": {
        "id": "OFCI2JM_oQeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute TF-IDF\n",
        "tfidf_dict = {}\n",
        "\n",
        "for key in stat_dict:\n",
        "    tfidf_dict[key] = stat_dict[key]*idf_dict[key]\n",
        "\n",
        "new_dict_mar = sorted(tfidf_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "print('依TF-IDF排序', '-'*50)\n",
        "print(new_dict_mar)"
      ],
      "metadata": {
        "id": "DJJNJIKCoSCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_text = dict(new_dict_mar)\n",
        "cloud = WordCloud(background_color = bg, width=width, height=height, font_path=myfont).generate_from_frequencies(dict_text)\n",
        "\n",
        "plt.imshow(cloud)"
      ],
      "metadata": {
        "id": "LT_dMJzAoUSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ---Apr---"
      ],
      "metadata": {
        "id": "NBQwbIHZoZ_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Content_LibTime = Strcontent_col_apr\n",
        "word_result = [item for sublist in having_GPT_apr for item in sublist]\n",
        "#print(word_result)\n",
        "text = ' '.join(word_result)\n",
        "\n",
        "# Compute TF\n",
        "stat_dict = {}\n",
        "\n",
        "for token in word_result:\n",
        "    if token in str_ann :\n",
        "        continue\n",
        "    if token in stat_dict:\n",
        "        stat_dict[token] = stat_dict[token] + 1\n",
        "    else:\n",
        "        stat_dict[token] = 1\n",
        "\n",
        "new_dict = sorted(stat_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "print('只依TF排序', '-'*50)\n",
        "print(new_dict)"
      ],
      "metadata": {
        "id": "xou9gYxAoWYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute IDF\n",
        "corpus_list = [Strcontent_col_jan, Strcontent_col_feb, Strcontent_col_mar, Strcontent_col_apr]\n",
        "\n",
        "idf_dict={}\n",
        "N=len(corpus_list)\n",
        "for key in stat_dict:\n",
        "    count=0\n",
        "    for sen in corpus_list:\n",
        "        if key in sen.split():\n",
        "            count=count+1\n",
        "        idf_dict[key]= math.log10(N / (float(count) + 1))\n",
        "\n",
        "print('只依IDF排序', '-'*50)\n",
        "new_dict = sorted(idf_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "print(new_dict)"
      ],
      "metadata": {
        "id": "EdD2uY2LoYUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute TF-IDF\n",
        "tfidf_dict = {}\n",
        "\n",
        "for key in stat_dict:\n",
        "    tfidf_dict[key] = stat_dict[key]*idf_dict[key]\n",
        "\n",
        "new_dict_apr = sorted(tfidf_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "print('依TF-IDF排序', '-'*50)\n",
        "print(new_dict_apr)"
      ],
      "metadata": {
        "id": "XwQdZoApoe0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_text = dict(new_dict_apr)\n",
        "cloud = WordCloud(background_color = bg, width=width, height=height, font_path=myfont).generate_from_frequencies(dict_text)\n",
        "\n",
        "plt.imshow(cloud)"
      ],
      "metadata": {
        "id": "I_PUfDcvog9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_keywords = {}\n",
        "filtered_keywords = {}\n",
        "filtered_keywords_above_5 = {}\n",
        "\n",
        "for month_index, lst in enumerate([new_dict_jan, new_dict_feb, new_dict_mar, new_dict_apr]):\n",
        "    for item in lst:\n",
        "        keyword = item[0]\n",
        "        value = item[1]\n",
        "        \n",
        "        if keyword not in all_keywords:\n",
        "            all_keywords[keyword] = [None, None, None, None]  \n",
        "        all_keywords[keyword][month_index] = value \n",
        "\n",
        "for keyword, values in all_keywords.items():\n",
        "    if None not in values:\n",
        "        filtered_keywords[keyword] = values\n",
        "\n",
        "for keyword, values in filtered_keywords.items():\n",
        "    if all(value >= 5 for value in values):\n",
        "        filtered_keywords_above_5[keyword] = values\n",
        "\n",
        "print(filtered_keywords)\n",
        "print(filtered_keywords_above_5)"
      ],
      "metadata": {
        "id": "a9RpyDbKoi5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(filtered_keywords)\n",
        "df = df.T\n",
        "df.columns = ['Jan','Feb','Mar','Apr']\n",
        "df"
      ],
      "metadata": {
        "id": "2k8RoEIXoneT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_label =  ['Jan','Feb','Mar','Apr']\n",
        "x = range(len(x_label))\n",
        "'''\n",
        "for i in filtered_keywords_above_5.keys():\n",
        "  plt.plot(x,filtered_keywords_above_5[i], label = 'i')\n",
        "'''\n",
        "for i in filtered_keywords.keys():\n",
        "    values = filtered_keywords[i]\n",
        "    if all(value >= 2.5 for value in values):\n",
        "      plt.plot(x,filtered_keywords[i], label = 'i')\n",
        "\n",
        "plt.xlabel('Month', fontsize=\"15\")\n",
        "plt.xticks(x,x_label)\n",
        "plt.ylabel('value', fontsize=\"15\")\n",
        "plt.title('techjob_TFIDF',fontsize=\"18\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OX-kuG1FordI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in filtered_keywords.keys():\n",
        "    values = filtered_keywords[i]\n",
        "    if all(value >= 2.5 for value in values):\n",
        "      print(i,values)"
      ],
      "metadata": {
        "id": "JFiIpOnDovzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B5WGTS3xp9FQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}