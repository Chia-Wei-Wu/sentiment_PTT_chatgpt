{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chia-Wei-Wu/sentiment_PTT_chatgpt/blob/main/textblob.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "JBL3aJAJNyFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import jieba\n",
        "import csv"
      ],
      "metadata": {
        "id": "t9ouMeQ5QTIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "i2XH64-iNiu1",
        "outputId": "cdb04d02-cd77-4eb9-c6fb-0426d9d33fc5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_frame = []\n",
        "data_frame = pd.DataFrame(columns=[\"title\",\"author\",\"date\",\"push\",\"url\",\"content\"])"
      ],
      "metadata": {
        "id": "sG-ZxYdNQCHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/tec_2023.json','r',encoding=\"utf-8\") as read_file:\n",
        "  lines = read_file.readlines()\n",
        "  for line in lines:\n",
        "    data_dic = json.loads(line)\n",
        "    data_row = pd.DataFrame([data_dic])\n",
        "    data_frame = pd.concat([data_frame, data_row],ignore_index=True)\n",
        "#print(data_frame)"
      ],
      "metadata": {
        "id": "X5s6YOxwQCKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classification by date\n",
        "data_frame[\"date\"] = pd.to_datetime(data_frame[\"date\"], format=\" %m/%d\")\n",
        "Jan_DF = []\n",
        "Feb_DF = []\n",
        "Mar_DF = []\n",
        "Apr_DF = []\n",
        "count_jan = 0\n",
        "count_feb = 0\n",
        "count_mar = 0\n",
        "count_apr = 0\n",
        "for index, row in data_frame.iterrows():\n",
        "  if row[\"date\"].month == 1:\n",
        "    Jan_DF.append(row)\n",
        "    count_jan += 1\n",
        "  elif row[\"date\"].month == 2:\n",
        "    Feb_DF.append(row)\n",
        "    count_feb += 1\n",
        "  elif row[\"date\"].month == 3:\n",
        "    Mar_DF.append(row)\n",
        "    count_mar += 1\n",
        "  elif row[\"date\"].month == 4:\n",
        "    Apr_DF.append(row)\n",
        "    count_apr += 1"
      ],
      "metadata": {
        "id": "9gieQ4c8QCMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(count_jan,count_feb,count_mar,count_apr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aS90yeasBcv",
        "outputId": "3678d429-c7f6-46b8-8c33-e5324bbfb4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "415 454 458 416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with Jan\n",
        "content_col_jan = []\n",
        "for row in Jan_DF:\n",
        "  content_col_jan.append(row[\"content\"])\n",
        "\n",
        "result_content_jan = []\n",
        "for row in content_col_jan:\n",
        "  row_content_jan = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_jan.append(word)\n",
        "  result_content_jan.append(row_content_jan)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_jan = []\n",
        "count_jan_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_jan:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_jan.append(row)\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "sentences_Jan = []\n",
        "for sublist in having_GPT_jan:\n",
        "  sentence = ''.join(sublist)\n",
        "  blob = TextBlob()\n",
        "  result = blob.sentences(sentence).Sentiment\n",
        "  print(result)\n",
        "  #print(result,sentence)\n",
        "#   label = \"\"\n",
        "#   if result[\"pos\"] > result[\"neg\"] :\n",
        "#     label = \"pos\"\n",
        "#   elif result[\"pos\"] == result[\"neg\"] :\n",
        "#     label = \"neu\"\n",
        "#   else:\n",
        "#     label = \"neg\"\n",
        "#   sentences_Jan.append([result,sentence,label])\n",
        "\n",
        "# with open('output_Jan_tec.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "#     writer = csv.writer(csvfile)\n",
        "#     writer.writerow(['Result', 'Sentence', 'label'])  \n",
        "#     writer.writerows(sentences_Jan)"
      ],
      "metadata": {
        "id": "aRmSqOTsQ4yt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "d21e31a5-6ec4-4a62-c7fe-5978bff54378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.509 seconds.\n",
            "DEBUG:jieba:Loading model cost 1.509 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b4a248190b10>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhaving_GPT_jan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: BaseBlob.__init__() missing 1 required positional argument: 'text'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with Feb\n",
        "content_col_feb = []\n",
        "for row in Feb_DF:\n",
        "  content_col_feb.append(row[\"content\"])\n",
        "\n",
        "result_content_feb = []\n",
        "for row in content_col_feb:\n",
        "  row_content_feb = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_feb.append(word)\n",
        "  result_content_feb.append(row_content_feb)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_feb = []\n",
        "count_feb_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_feb:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_feb.append(row)\n",
        "    count_feb_GPT += 1\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "senti = Sentiment()\n",
        "sentences_Feb = []\n",
        "for sublist in having_GPT_feb:\n",
        "  sentence = ''.join(sublist)\n",
        "  result = senti.sentiment_count(sentence)\n",
        "  #print(result,sentence)\n",
        "  label = \"\"\n",
        "  if result[\"pos\"] > result[\"neg\"] :\n",
        "    label = \"pos\"\n",
        "  elif result[\"pos\"] == result[\"neg\"] :\n",
        "    label = \"neu\"\n",
        "  else:\n",
        "    label = \"neg\"\n",
        "  sentences_Feb.append([result,sentence,label])\n",
        "\n",
        "with open('output_Feb_tec.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence', 'label'])  \n",
        "    writer.writerows(sentences_Feb)"
      ],
      "metadata": {
        "id": "tScDqeB5YdY2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "229a1f78-3c02-4b3f-baec-99b025129328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-9c1981f258ce>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#find whole sentence with Sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0msenti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0msentences_Feb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhaving_GPT_feb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Sentiment' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with mar\n",
        "content_col_mar = []\n",
        "for row in Mar_DF:\n",
        "  content_col_mar.append(row[\"content\"])\n",
        "\n",
        "result_content_mar = []\n",
        "for row in content_col_mar:\n",
        "  row_content_mar = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_mar.append(word)\n",
        "  result_content_mar.append(row_content_mar)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_mar = []\n",
        "count_mar_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_mar:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_mar.append(row)\n",
        "    count_mar_GPT += 1\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "senti = Sentiment()\n",
        "sentences_Mar = []\n",
        "for sublist in having_GPT_mar:\n",
        "  sentence = ''.join(sublist)\n",
        "  result = senti.sentiment_count(sentence)\n",
        "  #print(result,sentence)\n",
        "  label = \"\"\n",
        "  if result[\"pos\"] > result[\"neg\"] :\n",
        "    label = \"pos\"\n",
        "  elif result[\"pos\"] == result[\"neg\"] :\n",
        "    label = \"neu\"\n",
        "  else:\n",
        "    label = \"neg\"\n",
        "  sentences_Mar.append([result,sentence,label])\n",
        "\n",
        "with open('output_Mar_tec.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence', 'label'])  \n",
        "    writer.writerows(sentences_Mar)"
      ],
      "metadata": {
        "id": "KCz223T3dg6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with apr\n",
        "content_col_apr = []\n",
        "for row in Apr_DF:\n",
        "  content_col_apr.append(row[\"content\"])\n",
        "\n",
        "result_content_apr = []\n",
        "for row in content_col_apr:\n",
        "  row_content_apr = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_apr.append(word)\n",
        "  result_content_apr.append(row_content_apr)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_apr = []\n",
        "count_apr_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_apr:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_apr.append(row)\n",
        "    count_apr_GPT += 1\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "senti = Sentiment()\n",
        "sentences_Apr = []\n",
        "for sublist in having_GPT_apr:\n",
        "  sentence = ''.join(sublist)\n",
        "  result = senti.sentiment_count(sentence)\n",
        "  #print(result,sentence)\n",
        "  label = \"\"\n",
        "  if result[\"pos\"] > result[\"neg\"] :\n",
        "    label = \"pos\"\n",
        "  elif result[\"pos\"] == result[\"neg\"] :\n",
        "    label = \"neu\"\n",
        "  else:\n",
        "    label = \"neg\"\n",
        "  sentences_Apr.append([result,sentence,label])\n",
        "\n",
        "with open('output_Apr_tec.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence', 'label'])  \n",
        "    writer.writerows(sentences_Apr)"
      ],
      "metadata": {
        "id": "NEeIgEc1d27Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PTT Stock Sentiment"
      ],
      "metadata": {
        "id": "-d28xuYQejzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_frame = []\n",
        "data_frame = pd.DataFrame(columns=[\"title\",\"author\",\"date\",\"push\",\"url\",\"content\"])"
      ],
      "metadata": {
        "id": "cK9_zXbde0XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/stock_2023.json','r',encoding=\"utf-8\") as read_file:\n",
        "  lines = read_file.readlines()\n",
        "  for line in lines:\n",
        "    data_dic = json.loads(line)\n",
        "    data_row = pd.DataFrame([data_dic])\n",
        "    data_frame = pd.concat([data_frame, data_row],ignore_index=True)\n",
        "#print(data_frame)"
      ],
      "metadata": {
        "id": "BD6gwKokeqYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classification by date\n",
        "data_frame[\"date\"] = pd.to_datetime(data_frame[\"date\"], format=\" %m/%d\")\n",
        "Jan_DF = []\n",
        "Feb_DF = []\n",
        "Mar_DF = []\n",
        "Apr_DF = []\n",
        "count_jan = 0\n",
        "count_feb = 0\n",
        "count_mar = 0\n",
        "count_apr = 0\n",
        "for index, row in data_frame.iterrows():\n",
        "  if row[\"date\"].month == 1:\n",
        "    Jan_DF.append(row)\n",
        "    count_jan += 1\n",
        "  elif row[\"date\"].month == 2:\n",
        "    Feb_DF.append(row)\n",
        "    count_feb += 1\n",
        "  elif row[\"date\"].month == 3:\n",
        "    Mar_DF.append(row)\n",
        "    count_mar += 1\n",
        "  elif row[\"date\"].month == 4:\n",
        "    Apr_DF.append(row)\n",
        "    count_apr += 1"
      ],
      "metadata": {
        "id": "sTn_Ug0Ge4XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with Jan\n",
        "content_col_jan = []\n",
        "for row in Jan_DF:\n",
        "  content_col_jan.append(row[\"content\"])\n",
        "\n",
        "result_content_jan = []\n",
        "for row in content_col_jan:\n",
        "  row_content_jan = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_jan.append(word)\n",
        "  result_content_jan.append(row_content_jan)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_jan = []\n",
        "count_jan_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_jan:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_jan.append(row)\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "senti = Sentiment()\n",
        "sentences_Jan = []\n",
        "for sublist in having_GPT_jan:\n",
        "  sentence = ''.join(sublist)\n",
        "  result = senti.sentiment_count(sentence)\n",
        "  #print(result,sentence)\n",
        "  label = \"\"\n",
        "  if result[\"pos\"] > result[\"neg\"] :\n",
        "    label = \"pos\"\n",
        "  elif result[\"pos\"] == result[\"neg\"] :\n",
        "    label = \"neu\"\n",
        "  else:\n",
        "    label = \"neg\"\n",
        "  sentences_Jan.append([result,sentence,label])\n",
        "\n",
        "with open('output_Jan_stock.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence', 'label'])  \n",
        "    writer.writerows(sentences_Jan)"
      ],
      "metadata": {
        "id": "4d69QiXkfMSX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "25c84c1e-0934-455a-b5f6-a3a9c4ad19d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e0db729878e0>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#find whole sentence with Sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0msenti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0msentences_Jan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhaving_GPT_jan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Sentiment' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with Feb\n",
        "content_col_feb = []\n",
        "for row in Feb_DF:\n",
        "  content_col_feb.append(row[\"content\"])\n",
        "\n",
        "result_content_feb = []\n",
        "for row in content_col_feb:\n",
        "  row_content_feb = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_feb.append(word)\n",
        "  result_content_feb.append(row_content_feb)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_feb = []\n",
        "count_feb_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_feb:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_feb.append(row)\n",
        "    count_feb_GPT += 1\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "senti = Sentiment()\n",
        "sentences_Feb = []\n",
        "for sublist in having_GPT_feb:\n",
        "  sentence = ''.join(sublist)\n",
        "  result = senti.sentiment_count(sentence)\n",
        "  #print(result,sentence)\n",
        "  label = \"\"\n",
        "  if result[\"pos\"] > result[\"neg\"] :\n",
        "    label = \"pos\"\n",
        "  elif result[\"pos\"] == result[\"neg\"] :\n",
        "    label = \"neu\"\n",
        "  else:\n",
        "    label = \"neg\"\n",
        "  sentences_Feb.append([result,sentence,label])\n",
        "\n",
        "with open('output_Feb_stock.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence', 'label'])  \n",
        "    writer.writerows(sentences_Feb)"
      ],
      "metadata": {
        "id": "ISfJAk2af-lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with mar\n",
        "content_col_mar = []\n",
        "for row in Mar_DF:\n",
        "  content_col_mar.append(row[\"content\"])\n",
        "\n",
        "result_content_mar = []\n",
        "for row in content_col_mar:\n",
        "  row_content_mar = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_mar.append(word)\n",
        "  result_content_mar.append(row_content_mar)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_mar = []\n",
        "count_mar_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_mar:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_mar.append(row)\n",
        "    count_mar_GPT += 1\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "senti = Sentiment()\n",
        "sentences_Mar = []\n",
        "for sublist in having_GPT_mar:\n",
        "  sentence = ''.join(sublist)\n",
        "  result = senti.sentiment_count(sentence)\n",
        "  #print(result,sentence)\n",
        "  label = \"\"\n",
        "  if result[\"pos\"] > result[\"neg\"] :\n",
        "    label = \"pos\"\n",
        "  elif result[\"pos\"] == result[\"neg\"] :\n",
        "    label = \"neu\"\n",
        "  else:\n",
        "    label = \"neg\"\n",
        "  sentences_Mar.append([result,sentence,label])\n",
        "\n",
        "with open('output_Mar_stock.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence', 'label'])  \n",
        "    writer.writerows(sentences_Mar)"
      ],
      "metadata": {
        "id": "Hg98Lmh9gIBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with apr\n",
        "content_col_apr = []\n",
        "for row in Apr_DF:\n",
        "  content_col_apr.append(row[\"content\"])\n",
        "\n",
        "result_content_apr = []\n",
        "for row in content_col_apr:\n",
        "  row_content_apr = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_apr.append(word)\n",
        "  result_content_apr.append(row_content_apr)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_apr = []\n",
        "count_apr_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_apr:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_apr.append(row)\n",
        "    count_apr_GPT += 1\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "senti = Sentiment()\n",
        "sentences_Apr = []\n",
        "for sublist in having_GPT_apr:\n",
        "  sentence = ''.join(sublist)\n",
        "  result = senti.sentiment_count(sentence)\n",
        "  #print(result,sentence)\n",
        "  label = \"\"\n",
        "  if result[\"pos\"] > result[\"neg\"] :\n",
        "    label = \"pos\"\n",
        "  elif result[\"pos\"] == result[\"neg\"] :\n",
        "    label = \"neu\"\n",
        "  else:\n",
        "    label = \"neg\"\n",
        "  sentences_Apr.append([result,sentence,label])\n",
        "\n",
        "with open('output_Apr_stock.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence', 'label'])  \n",
        "    writer.writerows(sentences_Apr)"
      ],
      "metadata": {
        "id": "sVVOL4y4gMfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PDWZgvMJgQRO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}