{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPc/ituKUyTbl6ZaCOLJSbj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chia-Wei-Wu/sentiment_PTT_chatgpt/blob/main/textblob.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBL3aJAJNyFG",
        "outputId": "3d6311f4-d265-4432-ab16-ce9dbad17594"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import jieba\n",
        "import csv"
      ],
      "metadata": {
        "id": "t9ouMeQ5QTIQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2XH64-iNiu1",
        "outputId": "4091f545-d792-496d-c740-db9a0f09071a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_frame = []\n",
        "data_frame = pd.DataFrame(columns=[\"title\",\"author\",\"date\",\"push\",\"url\",\"content\"])"
      ],
      "metadata": {
        "id": "sG-ZxYdNQCHK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/AI_Final/tec_2023.json','r',encoding=\"utf-8\") as read_file:\n",
        "  lines = read_file.readlines()\n",
        "  for line in lines:\n",
        "    data_dic = json.loads(line)\n",
        "    data_row = pd.DataFrame([data_dic])\n",
        "    data_frame = pd.concat([data_frame, data_row],ignore_index=True)\n",
        "#print(data_frame)"
      ],
      "metadata": {
        "id": "X5s6YOxwQCKL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classification by date\n",
        "data_frame[\"date\"] = pd.to_datetime(data_frame[\"date\"], format=\" %m/%d\")\n",
        "Jan_DF = []\n",
        "Feb_DF = []\n",
        "Mar_DF = []\n",
        "Apr_DF = []\n",
        "count_jan = 0\n",
        "count_feb = 0\n",
        "count_mar = 0\n",
        "count_apr = 0\n",
        "for index, row in data_frame.iterrows():\n",
        "  if row[\"date\"].month == 1:\n",
        "    Jan_DF.append(row)\n",
        "    count_jan += 1\n",
        "  elif row[\"date\"].month == 2:\n",
        "    Feb_DF.append(row)\n",
        "    count_feb += 1\n",
        "  elif row[\"date\"].month == 3:\n",
        "    Mar_DF.append(row)\n",
        "    count_mar += 1\n",
        "  elif row[\"date\"].month == 4:\n",
        "    Apr_DF.append(row)\n",
        "    count_apr += 1"
      ],
      "metadata": {
        "id": "9gieQ4c8QCMt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(count_jan,count_feb,count_mar,count_apr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aS90yeasBcv",
        "outputId": "915a03f9-fd26-4af9-87ea-f063a87a33c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "415 454 458 416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with Jan\n",
        "content_col_jan = []\n",
        "for row in Jan_DF:\n",
        "  content_col_jan.append(row[\"content\"])\n",
        "\n",
        "result_content_jan = []\n",
        "for row in content_col_jan:\n",
        "  row_content_jan = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_jan.append(word)\n",
        "  result_content_jan.append(row_content_jan)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_jan = []\n",
        "count_jan_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_jan:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_jan.append(row)\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "sentences_Jan = []\n",
        "for sublist in having_GPT_jan:\n",
        "  sentence = ''.join(sublist)\n",
        "  blob = TextBlob(sentence)\n",
        "  result = blob.sentences[0].sentiment\n",
        "  #print(result)\n",
        "  sentences_Jan.append([result,sentence])\n",
        "\n",
        "with open('output_Jan_tec.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence'])  \n",
        "    writer.writerows(sentences_Jan)"
      ],
      "metadata": {
        "id": "aRmSqOTsQ4yt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with Feb\n",
        "content_col_feb = []\n",
        "for row in Feb_DF:\n",
        "  content_col_feb.append(row[\"content\"])\n",
        "\n",
        "result_content_feb = []\n",
        "for row in content_col_feb:\n",
        "  row_content_feb = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_feb.append(word)\n",
        "  result_content_feb.append(row_content_feb)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_feb = []\n",
        "count_feb_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_feb:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_feb.append(row)\n",
        "    count_feb_GPT += 1\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "sentences_Feb = []\n",
        "for sublist in having_GPT_feb:\n",
        "  sentence = ''.join(sublist)\n",
        "  blob = TextBlob(sentence)\n",
        "  result = blob.sentences[0].sentiment\n",
        "  sentences_Feb.append([result,sentence])\n",
        "\n",
        "with open('output_Feb_tec.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence'])  \n",
        "    writer.writerows(sentences_Feb)"
      ],
      "metadata": {
        "id": "tScDqeB5YdY2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with mar\n",
        "content_col_mar = []\n",
        "for row in Mar_DF:\n",
        "  content_col_mar.append(row[\"content\"])\n",
        "\n",
        "result_content_mar = []\n",
        "for row in content_col_mar:\n",
        "  row_content_mar = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_mar.append(word)\n",
        "  result_content_mar.append(row_content_mar)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_mar = []\n",
        "count_mar_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_mar:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_mar.append(row)\n",
        "    count_mar_GPT += 1\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "sentences_Mar = []\n",
        "for sublist in having_GPT_mar:\n",
        "  sentence = ''.join(sublist)\n",
        "  blob = TextBlob(sentence)\n",
        "  result = blob.sentences[0].sentiment\n",
        "  sentences_Mar.append([result,sentence])\n",
        "\n",
        "with open('output_Mar_tec.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence'])  \n",
        "    writer.writerows(sentences_Mar)"
      ],
      "metadata": {
        "id": "KCz223T3dg6K"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with apr\n",
        "content_col_apr = []\n",
        "for row in Apr_DF:\n",
        "  content_col_apr.append(row[\"content\"])\n",
        "\n",
        "result_content_apr = []\n",
        "for row in content_col_apr:\n",
        "  row_content_apr = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_apr.append(word)\n",
        "  result_content_apr.append(row_content_apr)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_apr = []\n",
        "count_apr_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_apr:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_apr.append(row)\n",
        "    count_apr_GPT += 1\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "\n",
        "sentences_Apr = []\n",
        "for sublist in having_GPT_apr:\n",
        "  sentence = ''.join(sublist)\n",
        "  blob = TextBlob(sentence)\n",
        "  result = blob.sentences[0].sentiment\n",
        "  sentences_Apr.append([result,sentence])\n",
        "\n",
        "with open('output_Apr_tec.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence'])  \n",
        "    writer.writerows(sentences_Apr)"
      ],
      "metadata": {
        "id": "NEeIgEc1d27Q"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PTT Stock Sentiment"
      ],
      "metadata": {
        "id": "-d28xuYQejzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_frame = []\n",
        "data_frame = pd.DataFrame(columns=[\"title\",\"author\",\"date\",\"push\",\"url\",\"content\"])"
      ],
      "metadata": {
        "id": "cK9_zXbde0XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/AI_Final/stock_2023.json','r',encoding=\"utf-8\") as read_file:\n",
        "  lines = read_file.readlines()\n",
        "  for line in lines:\n",
        "    data_dic = json.loads(line)\n",
        "    data_row = pd.DataFrame([data_dic])\n",
        "    data_frame = pd.concat([data_frame, data_row],ignore_index=True)\n",
        "#print(data_frame)"
      ],
      "metadata": {
        "id": "BD6gwKokeqYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#classification by date\n",
        "data_frame[\"date\"] = pd.to_datetime(data_frame[\"date\"], format=\" %m/%d\")\n",
        "Jan_DF = []\n",
        "Feb_DF = []\n",
        "Mar_DF = []\n",
        "Apr_DF = []\n",
        "count_jan = 0\n",
        "count_feb = 0\n",
        "count_mar = 0\n",
        "count_apr = 0\n",
        "for index, row in data_frame.iterrows():\n",
        "  if row[\"date\"].month == 1:\n",
        "    Jan_DF.append(row)\n",
        "    count_jan += 1\n",
        "  elif row[\"date\"].month == 2:\n",
        "    Feb_DF.append(row)\n",
        "    count_feb += 1\n",
        "  elif row[\"date\"].month == 3:\n",
        "    Mar_DF.append(row)\n",
        "    count_mar += 1\n",
        "  elif row[\"date\"].month == 4:\n",
        "    Apr_DF.append(row)\n",
        "    count_apr += 1"
      ],
      "metadata": {
        "id": "sTn_Ug0Ge4XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with Jan\n",
        "content_col_jan = []\n",
        "for row in Jan_DF:\n",
        "  content_col_jan.append(row[\"content\"])\n",
        "\n",
        "result_content_jan = []\n",
        "for row in content_col_jan:\n",
        "  row_content_jan = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_jan.append(word)\n",
        "  result_content_jan.append(row_content_jan)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_jan = []\n",
        "count_jan_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_jan:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_jan.append(row)\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "sentences_Jan = []\n",
        "for sublist in having_GPT_jan:\n",
        "  sentence = ''.join(sublist)\n",
        "  blob = TextBlob(sentence)\n",
        "  result = blob.sentences[0].sentiment\n",
        "  sentences_Jan.append([result,sentence])\n",
        "\n",
        "with open('output_Jan_stock.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence'])  \n",
        "    writer.writerows(sentences_Jan)"
      ],
      "metadata": {
        "id": "4d69QiXkfMSX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with Feb\n",
        "content_col_feb = []\n",
        "for row in Feb_DF:\n",
        "  content_col_feb.append(row[\"content\"])\n",
        "\n",
        "result_content_feb = []\n",
        "for row in content_col_feb:\n",
        "  row_content_feb = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_feb.append(word)\n",
        "  result_content_feb.append(row_content_feb)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_feb = []\n",
        "count_feb_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_feb:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_feb.append(row)\n",
        "    count_feb_GPT += 1\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "sentences_Feb = []\n",
        "for sublist in having_GPT_feb:\n",
        "  sentence = ''.join(sublist)\n",
        "  blob = TextBlob(sentence)\n",
        "  result = blob.sentences[0].sentiment\n",
        "  sentences_Feb.append([result,sentence])\n",
        "\n",
        "with open('output_Feb_stock.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence'])  \n",
        "    writer.writerows(sentences_Feb)"
      ],
      "metadata": {
        "id": "ISfJAk2af-lw"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with mar\n",
        "content_col_mar = []\n",
        "for row in Mar_DF:\n",
        "  content_col_mar.append(row[\"content\"])\n",
        "\n",
        "result_content_mar = []\n",
        "for row in content_col_mar:\n",
        "  row_content_mar = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_mar.append(word)\n",
        "  result_content_mar.append(row_content_mar)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_mar = []\n",
        "count_mar_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_mar:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_mar.append(row)\n",
        "    count_mar_GPT += 1\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "sentences_Mar = []\n",
        "for sublist in having_GPT_mar:\n",
        "  sentence = ''.join(sublist)\n",
        "  blob = TextBlob(sentence)\n",
        "  result = blob.sentences[0].sentiment\n",
        "  sentences_Mar.append([result,sentence])\n",
        "\n",
        "with open('output_Mar_stock.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence'])  \n",
        "    writer.writerows(sentences_Mar)"
      ],
      "metadata": {
        "id": "Hg98Lmh9gIBU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing with apr\n",
        "content_col_apr = []\n",
        "for row in Apr_DF:\n",
        "  content_col_apr.append(row[\"content\"])\n",
        "\n",
        "result_content_apr = []\n",
        "for row in content_col_apr:\n",
        "  row_content_apr = []\n",
        "  words = jieba.cut(row)\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    row_content_apr.append(word)\n",
        "  result_content_apr.append(row_content_apr)\n",
        "\n",
        "#Find \"ChatGPT\"\n",
        "having_GPT_apr = []\n",
        "count_apr_GPT = 0\n",
        "chatgpt = \"chatgpt\"\n",
        "for row in result_content_apr:\n",
        "  if chatgpt in row:\n",
        "    having_GPT_apr.append(row)\n",
        "    count_apr_GPT += 1\n",
        "\n",
        "#find whole sentence with Sentiment\n",
        "sentences_Apr = []\n",
        "for sublist in having_GPT_apr:\n",
        "  sentence = ''.join(sublist)\n",
        "  blob = TextBlob(sentence)\n",
        "  result = blob.sentences[0].sentiment\n",
        "  sentences_Apr.append([result,sentence])\n",
        "\n",
        "with open('output_Apr_stock.csv','w',encoding='utf-8-sig', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['Result', 'Sentence'])  \n",
        "    writer.writerows(sentences_Apr)"
      ],
      "metadata": {
        "id": "sVVOL4y4gMfu"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}